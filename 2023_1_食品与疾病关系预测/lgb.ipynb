{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, copy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.metrics import roc_auc_score, f1_score, recall_score, precision_score, precision_recall_curve\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import StratifiedKFold, KFold, StratifiedGroupKFold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 2023\n",
    "SAVE_PATH = 'model'\n",
    "\n",
    "cat_feats = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_food: (348, 213)\n",
      "train_answer: (141636, 3)\n",
      "disease_feature1: (220, 997)\n",
      "disease_feature2: (301, 3182)\n",
      "disease_feature3: (392, 1454)\n",
      "testA_food: (115, 213)\n",
      "testA_submit: (46805, 3)\n",
      "\n",
      "train disease num: 407\n",
      "train food num: 348\n"
     ]
    }
   ],
   "source": [
    "DATA_PATH = 'data'\n",
    "train_food = pd.read_csv(os.path.join(DATA_PATH, '训练集', 'train_food.csv'))\n",
    "train_answer = pd.read_csv(os.path.join(DATA_PATH, '训练集', 'train_answer.csv'))\n",
    "disease_feature1 = pd.read_csv(os.path.join(DATA_PATH, '训练集', 'disease_feature1.csv'))\n",
    "disease_feature2 = pd.read_csv(os.path.join(DATA_PATH, '训练集', 'disease_feature2.csv'))\n",
    "disease_feature3 = pd.read_csv(os.path.join(DATA_PATH, '训练集', 'disease_feature3.csv'))\n",
    "\n",
    "testA_food = pd.read_csv(os.path.join(DATA_PATH, '初赛A榜测试集', 'preliminary_a_food.csv'))\n",
    "testA_submit = pd.read_csv(os.path.join(DATA_PATH, '初赛A榜测试集', 'preliminary_a_submit_sample.csv'))\n",
    "\n",
    "print(f'train_food: {train_food.shape}')\n",
    "print(f'train_answer: {train_answer.shape}')\n",
    "print(f'disease_feature1: {disease_feature1.shape}')\n",
    "print(f'disease_feature2: {disease_feature2.shape}')\n",
    "print(f'disease_feature3: {disease_feature3.shape}')\n",
    "\n",
    "print(f'testA_food: {testA_food.shape}')\n",
    "print(f'testA_submit: {testA_submit.shape}')\n",
    "print()\n",
    "print(f'train disease num: {train_answer.disease_id.nunique()}')\n",
    "print(f'train food num: {train_answer.food_id.nunique()}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "food_feats = [item for item in train_food.columns if item not in ['food_id']]\n",
    "tmp = train_food[food_feats].isna().sum()\n",
    "food_feats = tmp[tmp<250].index.tolist()\n",
    "\n",
    "train_food = train_food[['food_id'] + food_feats]\n",
    "train_answer = train_answer.merge(train_food, on='food_id', how='left')\n",
    "\n",
    "testA_food = testA_food[['food_id'] + food_feats]\n",
    "testA_submit = testA_submit.merge(testA_food, on='food_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "def tsvd(data, feats, n_components=10, name='tsvd', load=False):\n",
    "\n",
    "    tsvd = Pipeline([\n",
    "        ('std', StandardScaler()),\n",
    "        # ('tsvd', TruncatedSVD(n_components=n_components, n_iter=1000, random_state=SEED)),\n",
    "        ('pca', PCA(n_components=n_components, random_state=SEED))\n",
    "    ])\n",
    "    tsvd.fit(data[feats])\n",
    "    data_id = data['disease_id']\n",
    "    deal_data = pd.DataFrame(tsvd.transform(data[feats]), columns=[f'{name}_{i}' for i in range(n_components)])\n",
    "    deal_data.insert(0, 'disease_id', data['disease_id'])\n",
    "    return deal_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_disease_tsvd = 100\n",
    "disease_feature3 = tsvd(\n",
    "    disease_feature3, \n",
    "    [item for item in disease_feature3.columns if item not in ['disease_id']], \n",
    "    n_components=n_disease_tsvd, \n",
    "    name='disease3_tsvd'\n",
    ")\n",
    "\n",
    "train_answer = train_answer.merge(disease_feature3, on='disease_id', how='left')\n",
    "testA_submit = testA_submit.merge(disease_feature3, on='disease_id', how='left')\n",
    "\n",
    "\n",
    "disease_feature2 = tsvd(\n",
    "    disease_feature2, \n",
    "    [item for item in disease_feature2.columns if item not in ['disease_id']], \n",
    "    n_components=n_disease_tsvd, \n",
    "    name='disease2_tsvd'\n",
    ")\n",
    "\n",
    "train_answer = train_answer.merge(disease_feature2, on='disease_id', how='left')\n",
    "testA_submit = testA_submit.merge(disease_feature2, on='disease_id', how='left')\n",
    "\n",
    "\n",
    "disease_feature1 = tsvd(\n",
    "    disease_feature1, \n",
    "    [item for item in disease_feature1.columns if item not in ['disease_id']], \n",
    "    n_components=n_disease_tsvd, \n",
    "    name='disease1_tsvd'\n",
    ")\n",
    "\n",
    "train_answer = train_answer.merge(disease_feature1, on='disease_id', how='left')\n",
    "testA_submit = testA_submit.merge(disease_feature1, on='disease_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(407, 407)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_answer['disease_id_lbl'] = train_answer['disease_id'].apply(lambda x: int(x.split('_')[-1]))\n",
    "testA_submit['disease_id_lbl'] = testA_submit['disease_id'].apply(lambda x: int(x.split('_')[-1]))\n",
    "cat_feats += ['disease_id_lbl']\n",
    "train_answer['disease_id_lbl'].nunique(), testA_submit['disease_id_lbl'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['disease_id_lbl']\n"
     ]
    }
   ],
   "source": [
    "feats = [item for item in train_answer.columns if item not in ['food_id', 'disease_id', 'related']]\n",
    "print(cat_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_params = {\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective': 'binary', #mse mape\n",
    "    'metric': ['auc', 'binary_logloss'],\n",
    "    # 'max_depth': 6,\n",
    "    'num_leaves': 2 ** 4,\n",
    "    # 'num_leaves': 31,\n",
    "    # 'min_data_in_leaf': 50,\n",
    "    'lambda_l1': 0.5,  \n",
    "    'lambda_l2': 0.5,  \n",
    "    'feature_fraction': 0.8,  \n",
    "    'bagging_fraction': 0.8, \n",
    "    'bagging_freq': 5,  \n",
    "    'learning_rate': 0.01,  \n",
    "    'n_jobs': -1,\n",
    "    'verbose': -1,\n",
    "    \"device_type\": \"cpu\",\n",
    "    'feature_fraction_seed':SEED,\n",
    "    'bagging_seed':SEED,\n",
    "    'seed': SEED,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(141636, 390) (46805, 390) (141636,)\n"
     ]
    }
   ],
   "source": [
    "train_x = train_answer[feats]\n",
    "testA_x = testA_submit[feats]\n",
    "train_y = train_answer['related']\n",
    "group_x = train_answer['food_id']\n",
    "print(train_x.shape, testA_x.shape, train_y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_name = \"lgb\"\n",
    "task_params = {\"lgb\": lgb_params}[task_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kaihua\\AppData\\Roaming\\Python\\Python38\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "C:\\Users\\Kaihua\\AppData\\Roaming\\Python\\Python38\\site-packages\\lightgbm\\basic.py:1780: UserWarning: Overriding the parameters from Reference Dataset.\n",
      "  _log_warning('Overriding the parameters from Reference Dataset.')\n",
      "C:\\Users\\Kaihua\\AppData\\Roaming\\Python\\Python38\\site-packages\\lightgbm\\basic.py:1513: UserWarning: categorical_column in param dict is overridden.\n",
      "  _log_warning(f'{cat_alias} in param dict is overridden.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 2000 rounds\n",
      "[5000]\ttraining's auc: 0.993786\ttraining's binary_logloss: 0.0751627\tvalid_1's auc: 0.97312\tvalid_1's binary_logloss: 0.114773\n",
      "[10000]\ttraining's auc: 0.99905\ttraining's binary_logloss: 0.049638\tvalid_1's auc: 0.975973\tvalid_1's binary_logloss: 0.106903\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[10000]\ttraining's auc: 0.99905\ttraining's binary_logloss: 0.049638\tvalid_1's auc: 0.975973\tvalid_1's binary_logloss: 0.106903\n",
      "----------- 1\n",
      "Training until validation scores don't improve for 2000 rounds\n",
      "[5000]\ttraining's auc: 0.993818\ttraining's binary_logloss: 0.0752845\tvalid_1's auc: 0.974339\tvalid_1's binary_logloss: 0.112898\n",
      "[10000]\ttraining's auc: 0.99911\ttraining's binary_logloss: 0.0495448\tvalid_1's auc: 0.977143\tvalid_1's binary_logloss: 0.104689\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[10000]\ttraining's auc: 0.99911\ttraining's binary_logloss: 0.0495448\tvalid_1's auc: 0.977143\tvalid_1's binary_logloss: 0.104689\n",
      "----------- 2\n",
      "Training until validation scores don't improve for 2000 rounds\n",
      "[5000]\ttraining's auc: 0.993932\ttraining's binary_logloss: 0.0752328\tvalid_1's auc: 0.974263\tvalid_1's binary_logloss: 0.111092\n",
      "[10000]\ttraining's auc: 0.999101\ttraining's binary_logloss: 0.0497165\tvalid_1's auc: 0.976975\tvalid_1's binary_logloss: 0.103355\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[10000]\ttraining's auc: 0.999101\ttraining's binary_logloss: 0.0497165\tvalid_1's auc: 0.976975\tvalid_1's binary_logloss: 0.103355\n",
      "----------- 3\n",
      "Training until validation scores don't improve for 2000 rounds\n",
      "[5000]\ttraining's auc: 0.993682\ttraining's binary_logloss: 0.0753434\tvalid_1's auc: 0.972515\tvalid_1's binary_logloss: 0.114585\n",
      "[10000]\ttraining's auc: 0.99906\ttraining's binary_logloss: 0.0495599\tvalid_1's auc: 0.975814\tvalid_1's binary_logloss: 0.106092\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[10000]\ttraining's auc: 0.99906\ttraining's binary_logloss: 0.0495599\tvalid_1's auc: 0.975814\tvalid_1's binary_logloss: 0.106092\n",
      "----------- 4\n",
      "Training until validation scores don't improve for 2000 rounds\n",
      "[5000]\ttraining's auc: 0.993881\ttraining's binary_logloss: 0.0758739\tvalid_1's auc: 0.975648\tvalid_1's binary_logloss: 0.110682\n",
      "[10000]\ttraining's auc: 0.999101\ttraining's binary_logloss: 0.0499767\tvalid_1's auc: 0.978247\tvalid_1's binary_logloss: 0.102318\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[10000]\ttraining's auc: 0.999101\ttraining's binary_logloss: 0.0499767\tvalid_1's auc: 0.978247\tvalid_1's binary_logloss: 0.102318\n",
      "                name    importance\n",
      "389   disease_id_lbl  1.253623e+06\n",
      "23              N_61  9.400775e+04\n",
      "22              N_60  7.908509e+04\n",
      "12              N_33  7.670655e+04\n",
      "35              N_85  4.638029e+04\n",
      "6               N_14  3.740630e+04\n",
      "28              N_74  3.701893e+04\n",
      "3                N_6  3.331972e+04\n",
      "45             N_111  3.289379e+04\n",
      "81             N_198  3.226239e+04\n",
      "21              N_59  3.192659e+04\n",
      "88             N_211  2.916477e+04\n",
      "67             N_165  2.717369e+04\n",
      "15              N_42  2.672063e+04\n",
      "87             N_209  2.608541e+04\n",
      "76             N_193  2.513283e+04\n",
      "84             N_204  2.500899e+04\n",
      "44             N_106  2.419422e+04\n",
      "56             N_146  2.290768e+04\n",
      "17              N_49  2.282010e+04\n",
      "11              N_28  2.207428e+04\n",
      "80             N_197  2.160317e+04\n",
      "78             N_195  2.100446e+04\n",
      "189  disease2_tsvd_0  2.097803e+04\n",
      "9               N_20  1.908773e+04\n",
      "41             N_101  1.905458e+04\n",
      "42             N_104  1.763997e+04\n",
      "57             N_147  1.734283e+04\n",
      "7               N_17  1.724771e+04\n",
      "49             N_119  1.631756e+04\n"
     ]
    }
   ],
   "source": [
    "train_oof = np.zeros(len(train_y))\n",
    "test_pred = np.zeros(len(testA_x))\n",
    "fold_num = 5\n",
    "importance = 0\n",
    "kf = StratifiedKFold(n_splits=fold_num, shuffle=True, random_state=SEED)\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(train_x, train_y)):\n",
    "    print('-----------', fold)\n",
    "    train = lgb.Dataset(\n",
    "        train_x.loc[train_idx],\n",
    "        train_y.loc[train_idx],\n",
    "        categorical_feature=cat_feats\n",
    "    )\n",
    "    val = lgb.Dataset(\n",
    "        train_x.loc[val_idx],\n",
    "        train_y.loc[val_idx],\n",
    "        categorical_feature=cat_feats\n",
    "    )\n",
    "    model = lgb.train(task_params, train, valid_sets=[train, val], num_boost_round=10000,\n",
    "                        callbacks=[lgb.early_stopping(2000), lgb.log_evaluation(5000)])\n",
    "\n",
    "    train_oof[val_idx] += (model.predict(train_x.loc[val_idx]))\n",
    "    test_pred += (model.predict(testA_x))/fold_num\n",
    "    importance += model.feature_importance(importance_type='gain') / fold_num\n",
    "\n",
    "feats_importance = pd.DataFrame()\n",
    "feats_importance['name'] = feats\n",
    "feats_importance['importance'] = importance\n",
    "print(feats_importance.sort_values('importance', ascending=False)[:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prob_post_processing(train_oof, test_pred, threshold):\n",
    "    train_oof = 1/(1+np.exp((-train_oof + threshold)*3))\n",
    "    test_pred = 1/(1+np.exp((-test_pred + threshold)*3))\n",
    "    return train_oof, test_pred\n",
    "def Find_Optimal_Cutoff_F1(y, prob, verbose=False):\n",
    "    precision, recall, threshold = precision_recall_curve(y,prob)\n",
    "    y = 2*(precision*recall)/(precision+recall)\n",
    "    Youden_index = np.argmax(y)\n",
    "    optimal_threshold = threshold[Youden_index]\n",
    "    if verbose: print(\"optimal_threshold\", optimal_threshold)\n",
    "    return optimal_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test thres 0.2798317073629163\n",
      "optimal_threshold 0.5915536298320496\n"
     ]
    }
   ],
   "source": [
    "optimal_threshold = test_pred[test_pred.argsort()][-4572]\n",
    "print('test thres', optimal_threshold)\n",
    "train_oof, test_pred = prob_post_processing(train_oof, test_pred, optimal_threshold)\n",
    "optimal_threshold = Find_Optimal_Cutoff_F1(train_y, train_oof, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_score: 0.8898153040093908\n",
      "auc_score: 0.9768188900738383\n",
      "f1_score: 0.8028117179449433\n",
      "precision_score(查准率): 0.8336772911411756\n",
      "recall_score(查全率): 0.7741500461352828\n"
     ]
    }
   ],
   "source": [
    "y_pred = (train_oof >= optimal_threshold).astype(int)\n",
    "y_true = train_y\n",
    "\n",
    "auc_value = roc_auc_score(train_y, train_oof)\n",
    "f1_vlaue = f1_score(y_true, y_pred)\n",
    "p_value = precision_score(y_true, y_pred)\n",
    "r_value = recall_score(y_true, y_pred)\n",
    "\n",
    "print('total_score:', (auc_value+f1_vlaue)/2)\n",
    "print(\"auc_score:\", auc_value)\n",
    "print(\"f1_score:\", f1_vlaue)\n",
    "print(\"precision_score(查准率):\", p_value)\n",
    "print(\"recall_score(查全率):\", r_value)\n",
    "score_str = f\"{(auc_value+f1_vlaue)/2:.8f}_{auc_value:.5f}_{f1_vlaue:.5f}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_oof = pd.DataFrame({'food_id': train_answer['food_id'], 'disease_id': train_answer['disease_id'], 'related': train_answer['related'], 'pred': train_oof})\n",
    "train_oof.to_csv(f'results/lgb_oof_{score_str}.csv', index=False)\n",
    "\n",
    "test_pred = pd.DataFrame({'food_id': testA_submit['food_id'], 'disease_id': testA_submit['disease_id'], 'related_prob': test_pred})\n",
    "test_pred.to_csv(f'results/lgb_pre_{score_str}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "连接数: 4572\n"
     ]
    }
   ],
   "source": [
    "print('连接数:', (test_pred.related_prob>=0.5).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.11 ('AL')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "82785e3617b41da40eac9dc72b5795aea1d455d121b27bd2aa7c1fc59bb3871c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
